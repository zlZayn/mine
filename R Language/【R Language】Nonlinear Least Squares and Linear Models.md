**Least squares is the primary mathematical method for achieving linear fitting.**

## Basics

Two functions in R:

-   `nls()` (Nonlinear Least Squares):

```R
nls(y ~ mathematical_expression, data = dataset, start = list(param1 = initial_value1, param2 = initial_value2,...))
# Supports nonlinear functions (composed of f(x,β)), parameters must be explicitly defined! And provide initial values!
# Pure mathematical expressions, more flexible, does not support interaction term syntactic sugar
```

$$\hat{y}=\beta_{0}+f(x_{1},\beta_{1})+f(x_{2},\beta_{2})+\dots+f(x_{n},\beta_{n})$$

-   `lm()` (Linear Models; Linear Least Squares):

```R
lm(y ~ mathematical_expression, data = dataset)
# Linear form (only βx combinations), parameters cannot be explicitly written! Automatically generated by the model!
# Supports interaction term syntactic sugar ('*', ':')
```

$$\hat{y}=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}+\dots+\beta_{n}x_{n}$$

Matrix form:

$$\beta^T=\begin{bmatrix}  \beta_0 \\
\beta_1 \\
\vdots \\
\beta_n  
\end{bmatrix}, X= \begin{bmatrix}
1\\
 x_{1}\\
\vdots\\
 x_{n}\\
\end{bmatrix}; \hat{y}=\beta X$$

* * *

## `nls()` and `lm()` Can Transform Into Each Other

When a nonlinear model can be **mathematically transformed** into a linear form, the nonlinear regression problem solved using `nls()` is equivalent to the linear regression problem solved using `lm()`. **Both essentially solve the same least squares optimization problem** by **minimizing the residual sum of squares ($SS_{R}$)** to estimate parameters, i.e., minimizing:

$$SS_{R} = \sum_{i=1}^n (y_i - \hat{y}_i)^2$$

Where $y$ is the observed value and $\hat{y}$ is the model's predicted value.

## Illustrative Example

Using the automobile fuel consumption data (`mtcars`) as an example:

### Nonlinear Model

Fitting model: $\hat{mpg} = k\times\dfrac{1}{wt} + b$

Where $mpg$ is miles per gallon and $wt$ is vehicle weight.

```R
nlsfit <- nls(mpg ~ k / wt + b, mtcars, start = list(k = 1, b = 0))

summary(nlsfit)
```



![](https://pic1.zhimg.com/v2-30400738a70be104fefaf655cc935802_1440w.jpg)

$$\hat{mpg}=\frac{45.829}{wt}+4.386$$

```R
ggplot(mtcars, aes(wt, mpg)) +
	geom_point() +
	geom_line(aes(y = predict(nlsfit))) +
        geom_segment(aes(xend = wt, yend = predict(nlsfit)), color = "red")
```

![](https://pic2.zhimg.com/v2-7c001bc58147551ad16e125befe9dae3_1440w.jpg)

### Linear Model

By defining a new variable $wt_{2} = \dfrac{1}{wt}$, the original nonlinear model transforms into: $\hat{mpg} = k\times wt_{2} + b$

This now has a linear structure compatible with `lm()` syntax.

> The core definition of a linear model is that it **exhibits a linear relationship with respect to the model parameters (coefficients)**

```R
mtcars2 <- mtcars |>
	mutate(wt2 = 1 / wt)
lmfit <- lm(mpg ~ wt2, mtcars2)

summary(lmfit)
```

![](https://pica.zhimg.com/v2-30887033a8ee445acc560cd831f61c30_1440w.jpg)

$$\hat{mpg}=45.829\times wt_{2}+4.386$$

The estimated parameters (boxed) are exactly equal.

```R
ggplot(mtcars2, aes(wt2, mpg)) +
	geom_point() +
	geom_line(aes(y = predict(lmfit))) +
        geom_segment(aes(xend = wt2, yend = predict(lmfit)), color = "red")
```

![](https://pic3.zhimg.com/v2-608fe87f1e5f6b355c919db9fdd92e64_1440w.jpg)



This is equivalent to performing a mathematical transformation on the x-axis, and the results are completely equivalent.

* * *

## Summary

`nls()` and `lm()` can be transformed into each other. `nls()` is more flexible, while `lm()` is more concise.

`lm()` essentially solves for the coefficients of narrow-sense linear models through **linear least squares**.

**Both essentially solve the same least squares optimization problem**.